{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables broadcast son variables de sólo lectura que se mandan una sola vez a cada nodo en lugar de ser enviadas con cada una de las tareas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo implementamos variables broadcast?\n",
    "### Cómo se define la variable broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para definir una variable broadcast necesitamos llamarla a través del contexto de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val broadcastVar = sc.broadcast(Array(1, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo consultar la variable broadcast\n",
    "Para poder consultar su valor utilizamos el método value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo se destruye una variable broadcast\n",
    "Antes de destruir una variable broadcast se sugiere qitar su persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcastVar.unpersist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya después eliminarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcastVar.destroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escenario hipotético 1\n",
    "Supongamos que queremos contar los elementos gramaticales de un párrafo y tenemos un Mapa de cada palabra con su categoría gramatical.\n",
    "\n",
    "Para ello, supongamos que tenemos un diccionario como este (si nos va bien):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " val dictionary = Map((\"man\"-> \"noun\"), (\"is\"->\"verb\"),(\"mortal\"->\"adjective\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos también que tenemos una función que cuenta cuántas veces encuentra estos elementos gramaticales en nuestro párrafo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def getElementsCount(word :String, dictionary:Map[String,String]):(String,Int) = {\n",
    "dictionary.filter{ case (wording,wordType) => wording.equals((word))}.map(x => (x._2,1)).headOption.getOrElse((\"unknown\" -> 1)) //some dummy logic\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y usamos esa función para contar cada elemento gramatical encontrado con un set de datos definido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = sc.parallelize(Array(\"man\",\"is\",\"mortal\",\"mortal\",\"1234\",\"789\",\"456\",\"is\",\"man\"))\n",
    "val grammarElementCounts = words.map( word => getElementsCount(word,dictionary)).reduceByKey((x,y) => x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hizo lo que queríamos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((adjective,2), (noun,2), (unknown,3), (verb,2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammarElementCounts.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sí, pero, aunque a nivel local está bien, puede ser problemático cuando lo queramos implementar en el clúster porque se requeriría ese diccionario en cada tarea en todos y cada uno de los ejecutores (tal vez un diccionariod e este tamaño no importa mucho, pero qué va  apasar cuando tengamos un diccionario de miles o decenas o centenas de miles de elementos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con broadcast\n",
    "Tengo el mismo diccionario, sólo que tengo que convertirlo en una variable broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val broadCastDictionary = sc.broadcast(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos un poco nuestra función getElementsCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getElementsCount(word :String, dictionary:org.apache.spark.broadcast.Broadcast[Map[String,String]]):(String,Int) = {\n",
    "dictionary.value.filter{ case (wording,wordType) => wording.equals((word))}.map(x => (x._2,1)).headOption.getOrElse((\"unknown\" -> 1))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar del diccionario crudo pasamos el diccionario broadbast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = sc.parallelize(Array(\"man\",\"is\",\"mortal\",\"mortal\",\"1234\",\"789\",\"456\",\"is\",\"man\"))\n",
    "val grammarElementCounts = words.map( word => getElementsCount(word,broadCastDictionary)).reduceByKey((x,y) => x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((adjective,2), (noun,2), (unknown,3), (verb,2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammarElementCounts.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escenario hipotético 2\n",
    "Supongamos que queremos hacer un join con un RDD de mayor tamaño que otro (lo mismo funciona para Data Frames).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val names_df = spark.read.option(\"header\",\"true\").csv(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+----+\n",
      "|  Nombre|    Ap_p|     Ap_m|  id|\n",
      "+--------+--------+---------+----+\n",
      "|Libertad| Badillo|Hernandez| 001|\n",
      "|  Ursula| Fuentes|   Berain| 002|\n",
      "| Mariana| Orantes|   Garcia| 003|\n",
      "|Consuelo|  Quinto|Hernandez| 004|\n",
      "|   David|Esquivel|   Garcia| 005|\n",
      "|  Rafael| Vazquez|  Vazquez| 006|\n",
      "| Vincent|   Isaac|  Anturez|007 |\n",
      "+--------+--------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val address_df = spark.read.option(\"header\",\"true\").csv(\"address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----------+\n",
      "| id|broad|       spec|\n",
      "+---+-----+-----------+\n",
      "|001|india|      Dheli|\n",
      "|002|  usa|Springfield|\n",
      "|003|india|    Calcuta|\n",
      "+---+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val joined = names_df.join(address_df, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---------+-----+-----------+\n",
      "| id|  Nombre|   Ap_p|     Ap_m|broad|       spec|\n",
      "+---+--------+-------+---------+-----+-----------+\n",
      "|001|Libertad|Badillo|Hernandez|india|      Dheli|\n",
      "|002|  Ursula|Fuentes|   Berain|  usa|Springfield|\n",
      "|003| Mariana|Orantes|   Garcia|india|    Calcuta|\n",
      "+---+--------+-------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, con el join anterior tanto names como addresses sufrirían de un shuffling en el clúster para poder llevar a cabo el join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suponiendo que tenemos RDDs\n",
    "Podríamos mandar el menor RDD a cada ejecutor con cada tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val names = sc.textFile(\"namesRDD\").map(line => (line.split(\",\")(3),line))\n",
    "val addresses = sc.textFile(\"addressRDD\").map(line=>(line.split(\",\")(0),line))\n",
    "val addressesMap = addresses.collect().toMap\n",
    "val joined = names.map(v=>(v._2,(addressesMap(v._1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto no resuelve del todo la situación, ya que para cada tarea en cada nodo estamos mandando una gran cantidad de datos, lo cual hace este procedimiento ineficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Con variables broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinedBroad = names_df.join(broadcast(address_df), Seq(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+---------+-----+-----------+\n",
      "| id|  Nombre|   Ap_p|     Ap_m|broad|       spec|\n",
      "+---+--------+-------+---------+-----+-----------+\n",
      "|001|Libertad|Badillo|Hernandez|india|      Dheli|\n",
      "|002|  Ursula|Fuentes|   Berain|  usa|Springfield|\n",
      "|003| Mariana|Orantes|   Garcia|india|    Calcuta|\n",
      "+---+--------+-------+---------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedBroad.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al crear una variable broadcast, esta se mandará a cada nodo una sola vez, reduciendo así el tráfico en la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes teóricas y de los ejercicios\n",
    "* https://books.japila.pl/apache-spark-internals/apache-spark-internals/2.4.4/spark-broadcast.html\n",
    "* https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#broadcast-variables\n",
    "* https://blog.knoldus.com/broadcast-variables-in-spark-how-and-when-to-use-them/\n",
    "* http://www.prathapkudupublog.com/2018/06/accumulators-and-broadcast-variables-in.html\n",
    "* https://vishnuviswanath.com/spark_rdd_part2.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
